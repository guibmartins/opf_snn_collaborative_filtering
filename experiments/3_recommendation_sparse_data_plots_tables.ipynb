{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Reseting to the default seaborn theme\n",
    "sb.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['amzms', 'ml100k', 'mlls']\n",
    "datasets.sort()\n",
    "\n",
    "algorithms = ['opf', 'opf_snn', 'kmeans', 'user_knn', 'dbscan']\n",
    "col_name = {\"opf\": \"OPF\", \n",
    "            \"opf_snn\": \"OPF$_{SNN}$\", \n",
    "            \"kmeans\": \"$k$-Means\", \n",
    "            \"user_knn\": \"User-KNN\", \n",
    "            \"dbscan\": \"DBSCAN\"}\n",
    "dist_name = {\"cosine\": \"Cosine\", \n",
    "             \"euclidean\": \"Euclidean\", \n",
    "             \"jaccard\": \"Jaccard\", \n",
    "             \"pearson\": \"Pearson\", \n",
    "             \"squared_euclidean\": \"Squared Euclidean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_df(f_path: str, alg: str = ''):\n",
    "    \n",
    "    df = pd.read_csv(f_path, header=0)\n",
    "    df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    df[['algorithm']] = df[['algorithm']].applymap(lambda x: col_name.get(x))\n",
    "    df[['distance']] = df[['distance']].applymap(lambda x: dist_name.get(x))\n",
    "    \n",
    "    if alg in algorithms:\n",
    "        df = df[df['algorithm'] == col_name.get(alg)]\n",
    "        return df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot evaluation measure x distance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sb.set_context(\"poster\", font_scale=1.2)\n",
    "sb.set_style(style=\"whitegrid\", rc={\"context\": \"poster\", \"font_scale\": 3, \"grid.linestyle\": \"--\"})\n",
    "\n",
    "metric = 'rmse'\n",
    "plt_name = f\"opf_snn_dist_{metric}\"\n",
    "    \n",
    "for ds in datasets:\n",
    "    \n",
    "    df_snn = csv_to_df(f\"../out/{ds}_results.csv\", alg='opf_snn')\n",
    "    \n",
    "    g = sb.relplot(kind=\"line\", data=df_snn, x=\"n_neighbors\", y=metric, hue=\"distance\", style=\"distance\", markers=True,\n",
    "            palette=\"Set1\", height=6, aspect=1.6, legend='brief', facet_kws={'legend_out': False}, errorbar=('ci', 50))\n",
    "    \n",
    "    g.set(xlabel=\"Maximum number of neighbors ($k_{max}$)\", ylabel=metric.upper())\n",
    "    \n",
    "    sb.despine(offset=5, trim=False)\n",
    "    \n",
    "    # plt.title(ds.upper())\n",
    "    plt.tight_layout(pad=.4)\n",
    "    plt.legend(frameon=False, ncol=2, bbox_to_anchor=(0.48, 1.4), loc='upper center', borderaxespad=0)\n",
    "    plt.savefig(f\"./figs/{ds}_opf_snn_dist_{metric}.pdf\", format='pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_csv(ds_name: str, alg: str = ''):\n",
    "    \n",
    "    col_name = {\"opf\": \"OPF\", \n",
    "                \"opf_snn\": \"OPF$_{SNN}$\", \n",
    "                \"user_knn\": \"User-KNN\", \n",
    "                \"kmeans\": \"$k$-Means\",\n",
    "                \"dbscan\": \"DBSCAN\"}\n",
    "    \n",
    "    dist_name = {\"cosine\": \"Cosine\", \"euclidean\": \"Euclidean\", \n",
    "                 \"jaccard\": \"Jaccard\", \"pearson\": \"Pearson\", \n",
    "                 \"squared_euclidean\": \"Squared Euclidean\"}\n",
    "    \n",
    "    for f in os.scandir('/'.join(['..', out_dir])):\n",
    "\n",
    "        if '.csv' in f.name and 'old' not in f.name and ds in f.name:\n",
    "\n",
    "            # print(\"Reading '\", ds_name, \"' dataset...\")\n",
    "            _df = pd.read_csv(f.path, header=0)\n",
    "            _df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "            _df[['algorithm']] = _df[['algorithm']].applymap(lambda x: col_name.get(x))\n",
    "            _df[['distance']] = _df[['distance']].applymap(lambda x: dist_name.get(x))\n",
    "        \n",
    "    if alg in algorithms:\n",
    "        _df = _df[_df['algorithm'] == col_name.get(alg)]\n",
    "        return _df\n",
    "        \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation phase evaluation\n",
    "#### Generaring statistics regarding the results from sparse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_entry(cols='alg'):\n",
    "\n",
    "    if cols == 'metric':\n",
    "        return {v: \"\" for v in metric_map.values()}\n",
    "    \n",
    "    return {v: \"\" for v in algo_map.values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"out\"\n",
    "\n",
    "algo_map = {\"user_knn\": \"User-KNN\", \n",
    "            \"kmeans\": \"$k$-Means\", \n",
    "            \"opf\": \"OPF\", \n",
    "            \"opf_snn\": \"OPF$_{SNN}$\",\n",
    "            \"dbscan\": \"DBSCAN\"}\n",
    "\n",
    "metric_map = {\"mae\": \"MAE\", \"rmse\": \"RMSE\", \n",
    "              \"ndcg@1\": \"NDCG@1\", \"ndcg@5\": \"NDCG@5\", \"ndcg@10\": \"NDCG@10\", \n",
    "              \"precision@1\": \"PRECISION@1\", \"precision@5\": \"PRECISION@5\", \"precision@10\": \"PRECISION@10\",\n",
    "              \"recall@1\": \"RECALL@1\", \"recall@5\": \"RECALL@5\", \"recall@10\": \"RECALL@10\"}\n",
    "dataset_map = {\"amzms\": \"AMZ-MS\", \"ml100k\": \"ML-100K\", \"mlls\": \"ML-LS\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "approaches = list(algo_map.values())\n",
    "measures = list(metric_map.values())\n",
    "kmax = [30, 50, 60]\n",
    "distances = ['Jaccard', 'Cosine', 'Squared Euclidean']\n",
    "alg_prm = {datasets[i]: [kmax[i], distances[i]] for i in range(0, len(datasets))}\n",
    "\n",
    "tb_index = pd.MultiIndex.from_product([list(dataset_map.values()), approaches], names=[\"Dataset\", \"Approach\"])\n",
    "tb = pd.DataFrame(index=tb_index, columns=list(metric_map.values()))\n",
    "\n",
    "for ds in datasets:\n",
    "\n",
    "    df = read_df_from_csv(ds)\n",
    "        \n",
    "    for alg in approaches:\n",
    "        \n",
    "        # Create a empty dataframe row\n",
    "        row = create_new_entry(cols='metric')\n",
    "    \n",
    "        search_query = f\"n_neighbors == {alg_prm.get(ds)[0]} and distance == '{alg_prm.get(ds)[1]}'\"\n",
    "        \n",
    "        query = df.query(search_query).groupby('algorithm')[list(metric_map.keys())]\n",
    "        \n",
    "        for m_key, m_value in metric_map.items():\n",
    "        \n",
    "            mean = query.mean().loc[alg][m_key]\n",
    "            std = query.std().loc[alg][m_key]\n",
    "            row[m_value] = f\"{mean:.4f} \\pm {std:.4f}\"\n",
    "        \n",
    "        tb.loc[dataset_map.get(ds), alg] = row\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rating prediction average results\n",
    "df_rp = tb[measures[:2]]\n",
    "# df_rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ranking average results\n",
    "df_rank = tb[measures[2:5]]\n",
    "# df_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decision support average results\n",
    "df_ds = tb[measures[5:]]\n",
    "# df_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average results considering all evaluation measures and datasets\n",
    "tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_csv(ds_name: str, alg: str = ''):\n",
    "    \n",
    "    col_name = {\"opf\": \"OPF\", \"opf_snn\": \"OPF$_{SNN}$\", \n",
    "                \"user_knn\": \"User-KNN\", \"kmeans\": \"$k$-Means\",\n",
    "                \"dbscan\": \"DBSCAN\"}\n",
    "    dist_name = {\"cosine\": \"Cosine\", \"euclidean\": \"Euclidean\", \n",
    "                 \"jaccard\": \"Jaccard\", \"pearson\": \"Pearson\", \n",
    "                 \"squared_euclidean\": \"Squared Euclidean\"}\n",
    "    \n",
    "    for f in os.scandir('/'.join(['..', out_dir])):\n",
    "\n",
    "        if '.csv' in f.name and 'old' not in f.name and ds in f.name:\n",
    "\n",
    "            # print(\"Reading '\", ds_name, \"' dataset...\")\n",
    "            _df = pd.read_csv(f.path, header=0)\n",
    "            _df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "            _df[['algorithm']] = _df[['algorithm']].applymap(lambda x: col_name.get(x))\n",
    "            _df[['distance']] = _df[['distance']].applymap(lambda x: dist_name.get(x))\n",
    "        \n",
    "    if alg in algorithms:\n",
    "        _df = _df[_df['algorithm'] == col_name.get(alg)]\n",
    "        return _df\n",
    "    \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_to_metric = {value: key for key, value in metric_map.items()}\n",
    "col_measures = list(key_to_metric.values())\n",
    "np_algs = np.array(list(algo_map.keys()))\n",
    "\n",
    "alpha = 0.05 # Significance level of the test\n",
    "alt_hipothesis = 'greater' # Better result = Maximum result\n",
    "zmethod = 'wilcox'\n",
    "\n",
    "for ds in datasets:\n",
    "   \n",
    "    # Get the optimal parameters regarding dataset ds\n",
    "    kmax = alg_prm.get(ds)[0]\n",
    "    dist = alg_prm.get(ds)[1]\n",
    "    \n",
    "    print(\"\\nDataset:\", ds, \"  kmax:\", kmax, \"  Distance:\", dist)\n",
    "    # print(\"Kmax: \", kmax)\n",
    "    # print(\"Distance: \", dist)\n",
    "    \n",
    "    df = read_df_from_csv(ds)\n",
    "    \n",
    "    search_query = f\"n_neighbors == {alg_prm.get(ds)[0]} and distance == '{alg_prm.get(ds)[1]}'\"\n",
    "    df_algs = {key: df.query(f\"{search_query} and algorithm == '{algo_map[key]}'\")[col_measures] \n",
    "               for key in algo_map.keys()}\n",
    "    \n",
    "    for metric, metric_key in metric_map.items():\n",
    "        \n",
    "        # Select the arg function (min or max) based on current eval metric\n",
    "        if metric in ['mae', 'rmse']:\n",
    "            idx_best = np.argmin([df_algs[i][metric].mean() for i in np_algs])\n",
    "        else:\n",
    "            idx_best = np.argmax([df_algs[i][metric].mean() for i in np_algs])\n",
    "        \n",
    "        alg_best = np_algs[idx_best]\n",
    "        print(\"Best:\", alg_best)\n",
    "        \n",
    "        # Sets the control group (i.e., the best average result)\n",
    "        cg = df_algs[alg_best][metric].values\n",
    "        \n",
    "        test_groups = np.delete(np_algs, idx_best)\n",
    "\n",
    "        for alg_test in test_groups:\n",
    "            \n",
    "            print(\"Test\", alg_test, \"against\", alg_best, \"...\")\n",
    "            \n",
    "            tg = df_algs[alg_test][metric].values\n",
    "\n",
    "            stat, pvalue = stats.wilcoxon(cg, tg, zero_method=zmethod, alternative='two-sided')\n",
    "\n",
    "            # print(f\"Stats: {stat}, p-value: {pvalue: .4f}\")\n",
    "\n",
    "            if pvalue > alpha: \n",
    "                # print(\"There is no difference in the medians of A and B.\")\n",
    "                \n",
    "                # control group\n",
    "                x = tb.loc[dataset_map.get(ds), algo_map.get(alg_best)][metric_key]\n",
    "                \n",
    "                if \"$\" not in x:\n",
    "                    tb.loc[dataset_map.get(ds), algo_map.get(alg_best)][metric_key] = \"$\" + x + \"$\"\n",
    "                \n",
    "                # test group\n",
    "                x = tb.loc[dataset_map.get(ds), algo_map.get(alg_test)][metric_key]\n",
    "                \n",
    "                if \"$\" not in x:\n",
    "                    tb.loc[dataset_map.get(ds), algo_map.get(alg_test)][metric_key] = \"$\" + x + \"$\"\n",
    "                \n",
    "            else:\n",
    "                # print(\"There is a difference between the medians of A and B.\")\n",
    "\n",
    "                # Better result = minimum result\n",
    "                if metric in ['mae', 'rmse']:\n",
    "                    # print(\"Alternative = 'less'\")\n",
    "                    alt_hipothesis = 'less'\n",
    "                else:\n",
    "                    # print(\"Alternative = 'greater'\")\n",
    "                    alt_hipothesis = 'greater'\n",
    "\n",
    "                stat, pvalue = stats.wilcoxon(cg, tg, zero_method=zmethod, alternative=alt_hipothesis)\n",
    "\n",
    "                if pvalue > alpha:\n",
    "                    # Null Hipothesis is true: highlight the test group (tg) in bold\n",
    "                    # print(alg, \"is superior.\")\n",
    "                    \n",
    "                    # test group\n",
    "                    x = tb.loc[dataset_map.get(ds), algo_map.get(alg_test)][metric_key]\n",
    "                    \n",
    "                    if \"mathbf\" not in x:\n",
    "                        if \"$\" in x: x = x[1:-1]\n",
    "                        tb.loc[dataset_map.get(ds), algo_map.get(alg_test)][metric_key] = \"$\\mathbf{\" + x + \"}$\"\n",
    "                    \n",
    "                    # control group\n",
    "                    x = tb.loc[dataset_map.get(ds), algo_map.get(alg_best)][metric_key]\n",
    "                    \n",
    "                    if \"$\" not in x:\n",
    "                        tb.loc[dataset_map.get(ds), algo_map.get(alg_best)][metric_key] = \"$\" + x + \"$\"\n",
    "                        \n",
    "                else:\n",
    "                    # Alt. Hipothesis is true: highlight the control group (cg) in bold\n",
    "                    # print(np_algs[idx_best], \"is superior.\")\n",
    "                    \n",
    "                    # control group\n",
    "                    x = tb.loc[dataset_map.get(ds), algo_map.get(alg_best)][metric_key]\n",
    "                    \n",
    "                    if \"mathbf\" not in x:\n",
    "                        if \"$\" in x: x = x[1:-1]\n",
    "                        tb.loc[dataset_map.get(ds), algo_map.get(alg_best)][metric_key] = \"$\\mathbf{\" + x + \"}$\"\n",
    "\n",
    "                    # test group\n",
    "                    x = tb.loc[dataset_map.get(ds), algo_map.get(alg_test)][metric_key]\n",
    "                    \n",
    "                    if \"$\" not in x:\n",
    "                        tb.loc[dataset_map.get(ds), algo_map.get(alg_test)][metric_key] = \"$\" + x + \"$\"\n",
    "\n",
    "print(\"Done...\")\n",
    "\n",
    "# Saving the dataframe as .csv (will be converted to latex table further) \n",
    "save_path = \"/\".join([\"tables\", \"sparse\", f\"sparse_data_stats_22.06.2023.csv\"])\n",
    "tb.to_csv(save_path)\n",
    "\n",
    "print(\"Results latex table saved in '\", save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
